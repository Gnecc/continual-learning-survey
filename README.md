# Continual Learning Survey — Full Reference Repository

This repository accompanies the survey  
**“A Survey on Continual Learning and Catastrophic Forgetting”**.

It contains the **complete list of the 90 research works reviewed**, including foundational papers, representative methods, benchmarks, recent surveys, and extensions to domains such as NLP, multimodal learning, and large language models (LLMs).

All the works listed here **were considered during the analysis, categorization, and synthesis process** presented in the survey. However, due to space constraints, **not all papers are discussed explicitly in the main text**, and many are primarily used to support the comparative tables, methodological taxonomy, and overall conclusions.

# Continual Learning Survey — Full Reference Repository

Enumerated list of the **90** reviewed works (from `core90.csv`).

1. **Overcoming Catastrophic Forgetting in Neural Networks**  
   [[paper]](https://arxiv.org/abs/1612.00796)

2. **Continual Learning through Synaptic Intelligence**  
   [[paper]](https://arxiv.org/abs/1703.04200)

3. **Memory Aware Synapses**  
   [[paper]](https://arxiv.org/abs/1711.09601)

4. **Learning without Forgetting**  
   [[paper]](https://arxiv.org/abs/1606.09282)

5. **Embedding Bayesian Latent Learning**  
   [[paper]](https://arxiv.org/pdf/2401.13766v1)

6. **Progress & Compress**  
   [[paper]](https://arxiv.org/abs/1805.06370)

7. **Overcoming Catastrophic Forgetting via Fisher Information**  
   [[paper]](https://arxiv.org/pdf/1703.08475)

8. **Random Walk Fundamental Tensor and Graph Importance Measures**  
   [[paper]](https://www-users.cse.umn.edu/~boley/publications/papers/BSMDMA2019.pdf)

9. **Learning without Memorizing**  
   [[paper]](https://arxiv.org/abs/1811.08051)

10. **Computational models of learning and synaptic plasticity**  
   [[paper]](https://arxiv.org/pdf/2412.05501)

11. **Co2L: Contrastive Continual Learning**  
   [[paper]](https://arxiv.org/abs/2106.14413)

12. **PODNet**  
   [[paper]](https://arxiv.org/abs/2004.13513)

13. **Elastic Information Bottleneck for Continual Learning (ELI)**  
   [[paper]](https://arxiv.org/abs/2311.03955)

14. **Efficient Bayesian Updates for Deep Learning via Laplace Approximations**  
   [[paper]](https://arxiv.org/abs/2210.06112)

15. **Exemplar-free Continual Representation Learning via Learnable Drift Compensation**  
   [[paper]](https://arxiv.org/abs/2407.08536)

16. **Investigating Continual Pretraining in Large Language Models: Insights and Implications**  
   [[paper]](https://arxiv.org/abs/2402.17400)

17. **Dynamic Contrastive Knowledge Distillation for Efficient Image Restoration**  
   [[paper]](https://arxiv.org/abs/2412.08939)

18. **Subspace Distillation for Continual Learning**  
   [[paper]](https://arxiv.org/abs/2307.16419)

19. **iCaRL: Incremental Classifier and Representation Learning**  
   [[paper]](https://arxiv.org/abs/1611.07725)

20. **Gradient Episodic Memory (GEM)**  
   [[paper]](https://arxiv.org/abs/1706.08840)

21. **Average GEM**  
   [[paper]](https://arxiv.org/abs/1812.00420)

22. **Deep Generative Replay**  
   [[paper]](https://arxiv.org/abs/1705.08690)

23. **Learning a Unified Classifier Incrementally via Rebalancing**  
   [[paper]](https://sci-hub.se/https://ieeexplore.ieee.org/document/8953661)

24. **Dark Experience Replay**  
   [[paper]](https://arxiv.org/pdf/2004.07211)

25. **Continual Learning with Tiny Episodic Memories**  
   [[paper]](https://arxiv.org/pdf/1902.10486)

26. **Rainbow Memory: Efficient Replay for CL**  
   [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Bang_Rainbow_Memory_Continual_Learning_With_a_Memory_of_Diverse_Samples_CVPR_2021_paper.pdf)

27. **Memory Efficient Replay using k-Means Prototypes**  
   [[paper]](https://arxiv.org/abs/2004.00713)

28. **Memory Replay GANs**  
   [[paper]](https://arxiv.org/pdf/1809.02058)

29. **Continual Learning on Noisy Data Streams via Self-Purified Replay**  
   [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Continual_Learning_on_Noisy_Data_Streams_via_Self-Purified_Replay_ICCV_2021_paper.pdf)

30. **GDumb: A Simple Approach That Works Surprisingly Well**  
   [[paper]](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470511.pdf)

31. **Few-Shot Class-Incremental Learning**  
   [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Tao_Few-Shot_Class-Incremental_Learning_CVPR_2020_paper.pdf)

32. **Robust internal representations for domain generalization**  
   [[paper]](https://onlinelibrary.wiley.com/doi/10.1002/aaai.12137)

33. **Self-Supervised Models are Continual Learners**  
   [[paper]](https://arxiv.org/pdf/2112.04215)

34. **CLA: LATENT ALIGNMENT FOR ONLINE CONTINUAL**  
   [[paper]](https://arxiv.org/pdf/2507.10434)

35. **Scalable Strategies for Continual Learning with Replay**  
   [[paper]](https://arxiv.org/abs/2505.12512)

36. **ITL: Incremental Task Learning via Meta-Learning**  
   [[paper]](https://arxiv.org/abs/2003.11652)

37. **CURL: Contrastive Unsupervised Representations for Reinforcement Learning**  
   [[paper]](https://arxiv.org/abs/2004.04136)

38. **SS-IL++: Improved Self-Supervised Incremental Learning**  
   [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Ahn_SS-IL_Separated_Softmax_for_Incremental_Learning_ICCV_2021_paper.pdf)

39. **DualNet: Continual Learning, Fast and Slow**  
   [[paper]](https://arxiv.org/abs/2110.00175)

40. **Gradient Projection Memory (GPM)**  
   [[paper]](https://arxiv.org/abs/2103.09762)

41. **Exploring the Stability Gap in Continual Learning**  
   [[paper]](https://openaccess.thecvf.com/content/WACV2025/papers/Lapacz_Exploring_the_Stability_Gap_in_Continual_Learning_The_Role_of_WACV_2025_paper.pdf)

42. **Representational Continuity for Continual Learning**  
   [[paper]](https://arxiv.org/abs/2110.06976)

43. **Orthogonal Gradient Descent for Continual Learning**  
   [[paper]](https://arxiv.org/abs/2006.05964)

44. **Gradient Projection Memory for Continual Learning**  
   [[paper]](https://openreview.net/forum?id=BJge3TNKwH)

45. **Learning to Learn without Forgetting by Maximizing Transfer**  
   [[paper]](https://arxiv.org/pdf/1810.11910)

46. **Overcoming Catastrophic Interference by Conceptors**  
   [[paper]](https://arxiv.org/pdf/1707.04853)

47. **Optimization of path-integral tensor-multiplication schemes in open quantum systems**  
   [[paper]](https://arxiv.org/html/2502.15136v1)

48. **Understanding the Role of Learning Rate in CL**  
   [[paper]](https://arxiv.org/pdf/2006.06958)

49. **Adversarial Continual Learning**  
   [[paper]](https://arxiv.org/pdf/2003.09553)

50. **When Meta-Learning Meets Online and Continual Learning: A Survey**  
   [[paper]](https://arxiv.org/html/2311.05241v3)

51. **Personalized Federated Learning via Gradient Modulation for Heterogeneous Text Summarization**  
   [[paper]](https://arxiv.org/abs/2304.11524)

52. **Meta-Learning Representations for Continual Learning**  
   [[paper]](https://arxiv.org/abs/1905.12588)

53. **Progressive Neural Networks**  
   [[paper]](https://arxiv.org/abs/1606.04671)

54. **PackNet**  
   [[paper]](https://openaccess.thecvf.com/content_cvpr_2018/html/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.html)

55. **Piggyback**  
   [[paper]](https://arxiv.org/abs/1801.06519)

56. **Hard Attention to the Task (HAT)**  
   [[paper]](https://arxiv.org/abs/1801.01423)

57. **Dynamic Expansion Networks**  
   [[paper]](https://arxiv.org/abs/1708.01547)

58. **PathNet**  
   [[paper]](https://arxiv.org/abs/1701.08734)

59. **Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution**  
   [[paper]](https://arxiv.org/html/2510.01997v2)

60. **Avoiding Forgetting and Allowing Forward Transfer in Continual Learning via Sparse Networks**  
   [[paper]](https://arxiv.org/abs/2110.05329)

61. **Conditional Computation in CL**  
   [[paper]](https://arxiv.org/pdf/2004.00070)

62. **Dynamic Sparsity Networks for Incremental Tasks**  
   [[paper]](https://arxiv.org/pdf/1909.06964)

63. **IMPROVING DESIGN OF INPUT CONDITION INVARIANT SPEECH ENHANCEMENT**  
   [[paper]](arXiv 2024)

64. **AANets: Adaptive Aggregation Networks for Continual Learning**  
   [[paper]](https://arxiv.org/abs/2010.05063)

65. **PromptFusion: Cross-Modal Continual Learning with Fusion Prompts**  
   [[paper]](https://aclanthology.org/2023.repl4nlp-1.10/)

66. **Avalanche: An End-to-End Library for Continual Learning**  
   [[paper]](https://arxiv.org/abs/2104.00405)

67. **VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks**  
   [[paper]](https://arxiv.org/abs/2112.06825)

68. **CL-MoE: Enhancing Multimodal Large Language Model**  
   [[paper]](https://arxiv.org/html/2503.00413v1)

69. **Multi-modal Learning with Missing Modality via Shared-Specific Feature Modelling**  
   [[paper]](https://arxiv.org/abs/2307.14126)

70. **LADA: Scalable Label-Specific CLIP Adapter for Continual Learning**  
   [[paper]](https://arxiv.org/html/2505.23271v1)

71. **Continual Lifelong Learning with Neural Networks: A Review**  
   [[paper]](https://arxiv.org/abs/1802.07569)

72. **A Continual Learning Survey: Defying Forgetting**  
   [[paper]](https://arxiv.org/abs/1909.08383)

73. **Online Continual Learning: Challenges**  
   [[paper]](Benchmarks/Surveys)

74. **Embracing Change: Continual Learning in Deep Neural Networks**  
   [[paper]](https://www.sciencedirect.com/science/article/pii/S1364661320302199)

75. **Class-Incremental Learning: A Survey**  
   [[paper]](https://arxiv.org/html/2302.03648v2)

76. **Vision Language Transformers: A Survey**  
   [[paper]](https://arxiv.org/abs/2307.03254)

77. **CORe50: Benchmark for Continuous Object Recognition**  
   [[paper]](https://arxiv.org/pdf/1705.03550)

78. **Continual evaluation for lifelong learning: Identifying the stability gap**  
   [[paper]](https://arxiv.org/abs/2205.13452)

79. **The Future of continual learning**  
   [[paper]](https://arxiv.org/pdf/2506.03320)

80. **Episodic Memory in LSTM Language Models**  
   [[paper]](https://arxiv.org/abs/1906.01076)

81. **Continual Learning for Natural Language Processing: A Survey**  
   [[paper]](https://arxiv.org/pdf/2211.12701)

82. **LoRA: Low-Rank Adaptation of Large Language Models**  
   [[paper]](https://arxiv.org/pdf/2106.09685)

83. **Continual Learning for Large Language Models**  
   [[paper]](https://arxiv.org/pdf/2402.01364v1)

84. **LAMOL: Language Modeling for Lifelong Learning**  
   [[paper]](https://arxiv.org/abs/1909.03329)

85. **CONTINUAL PRE-TRAINING OF LANGUAGE MODELS**  
   [[paper]](https://arxiv.org/pdf/2302.03241)

86. **Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents**  
   [[paper]](https://arxiv.org/pdf/2502.06975)

87. **Few-shot Continual Learning: a Brain-inspired Approach**  
   [[paper]](https://arxiv.org/abs/2104.09034)

88. **Visual Prompt Tuning in Null Space for Continual Learning**  
   [[paper]](https://arxiv.org/pdf/2406.05658)

89. **Dynamic Integration of Task-Specific Adapters for Class Incremental Learning**  
   [[paper]](https://arxiv.org/abs/2409.14983)

90. **Class-Incremental Learning: A Survey**  
   [[paper]](https://arxiv.org/pdf/2302.03648)


### Final Note

This repository represents the complete corpus of works reviewed in the survey.
It is intended as supplementary material to ensure transparency, traceability, and reproducibility.
