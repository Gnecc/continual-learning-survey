Autor principal,Año,Título,Estrategia / Tipo,Categoría,Fuente / Enlace,Resultados clave,Metricas,Datasets,
Kirkpatrick,2017,Overcoming Catastrophic Forgetting in Neural Networks,EWC,Regularización,https://arxiv.org/abs/1612.00796,Penaliza cambios en pesos importantes usando Fisher Information.,"Accuracy, Forgetting","Permuted MNIST, Split MNIST",
Zenke,2017,Continual Learning through Synaptic Intelligence,SI,Regularización,https://arxiv.org/abs/1703.04200,Asigna importancia dinámica a pesos según su contribución al desempeño previo.,"Accuracy, Task performance","Permuted MNIST, Split MNIST",
Aljundi,2018,Memory Aware Synapses,MAS,Regularización,https://arxiv.org/abs/1711.09601,"Evalúa sensibilidad de salida para regularizar parámetros relevantes.,,Aprendizaje incremental sin etiquetas de tarea.","Accuracy, Forgetting","Split MNIST, Permuted MNIST",
Li & Hoiem,2016,Learning without Forgetting,LwF,Regularización,https://arxiv.org/abs/1606.09282,"Top-,Usa distillation para preservar salidas antiguas mientras entrena nuevas tareas.","Average Accuracy, Backward Transfer","CIFAR-100, ImageNet",
Ke - HU,2022,Embedding Bayesian Latent Learning,EBLL,Regularización,https://arxiv.org/pdf/2401.13766v1,Modela representaciones latentes bayesianas para estabilidad y plasticidad,"Accuracy, Perplexity","GLUE, XSum, CoNLL-2003",
Schwarz,2018,Progress & Compress,EWC + Distillation,Regularización,https://arxiv.org/abs/1805.06370,Combina distillation con EWC para aprendizaje secuencial eficiente.,"Accuracy, Average Retained Accuracy","Split MNIST, Permuted MNIST",
Lee,2017,Overcoming Catastrophic Forgetting via Fisher Information,EWC Variant,Regularización,https://arxiv.org/pdf/1703.08475,Propone cálculo mejorado del Fisher para estabilidad inter-tareas.,Accuracy,"CIFAR-10, CIFAR-100",
Boley,2019,Random Walk Fundamental Tensor and Graph Importance Measures,RWalk,Regularización,https://www-users.cse.umn.edu/~boley/publications/papers/BSMDMA2019.pdf,Propone un tensor fundamental para calcular medidas de importancia en grafos dirigidos de forma eficiente y escalable.,"Centrality, Betweenness, Hitting Time","Synthetic Graphs, Scale-Free Networks",
Dhar,2019,Learning without Memorizing,LwMem,Regularización,https://arxiv.org/abs/1811.08051,Regularización ligera para tareas sucesivas sin almacenar ejemplos.,"Accuracy, Forgetting","CIFAR-100, ImageNet",
Tyulmankov,2024,Computational models of learning and synaptic plasticity,ESL,Regularización,https://arxiv.org/pdf/2412.05501,Desarrolla aprendizaje continuo para NLP que conserva conocimiento previo y se adapta a nuevos dominios lingüísticos.,Accuracy,Split MNIST,
Cha et al.,2022,Co2L: Contrastive Continual Learning,Contrastive Representation Regularization,Regularization,https://arxiv.org/abs/2106.14413,Contraste positivo/negativo; reduce interferencia.,"ACC, Forgetting %",CIFAR-100,
Douillard et al.,2020,PODNet,Distillation sobre representaciones intermedias,Regularization,https://arxiv.org/abs/2004.13513,,"Top-1 Acc, _old",CIFAR-100,
Ni et al,2023,Elastic Information Bottleneck for Continual Learning (ELI),Regularization (Information Bottleneck),Regularization,https://arxiv.org/abs/2311.03955,Minimiza pérdida de información irrelevante mediante compresión adaptativa.,ACC; BWT,CIFAR-100; ImageNet,
Huseljic et al.,2024,Efficient Bayesian Updates for Deep Learning via Laplace Approximations,Variational Bayesian Regularization,Regularization,https://arxiv.org/abs/2210.06112,Inferencia bayesiana eficiente que evita olvido catastrófico.,"ACC, BWT","Synthetic, MNIST",
Gomez-Villa et al.,2024,Exemplar-free Continual Representation Learning via Learnable Drift Compensation,Memory-free Regularization,Regularization,https://arxiv.org/abs/2407.08536,Sin replay; imprime memoria distribuida a nivel de capas.,"ACC, BWT",CIFAR-100,
Yildiz et al.,2025,Investigating Continual Pretraining in Large Language Models: Insights and Implications,Distillation + Data Augmentation,Regularization,https://arxiv.org/abs/2402.17400,Integra distillation adaptativa con aumento de datos dinámico; +6 % retención.,ACC; BWT,CIFAR-100,
Zhoe et al.,2024,Dynamic Contrastive Knowledge Distillation for Efficient Image Restoration,Contrastive + Distillation Regularization,Regularization,https://arxiv.org/abs/2412.08939,Alinea embeddings nuevos con antiguos usando pérdida contrastiva.,ACC; Forgetting %,CIFAR-100,
Roy et al.,2023,Subspace Distillation for Continual Learning,Feature Distillation (Geometric),Regularization,https://arxiv.org/abs/2307.16419,Usa distancias geodésicas para preservar estructura del espacio latente.,ACC; BWT,CIFAR-100,
Rebuffi,2017,iCaRL: Incremental Classifier and Representation Learning,iCaRL,Replay,https://arxiv.org/abs/1611.07725,Combina replay con distillation para clasificación incremental.,"Accuracy, Mean Class Accuracy","iCIFAR-100, iImageNet",
Lopez-Paz,2017,Gradient Episodic Memory (GEM),GEM,Replay,https://arxiv.org/abs/1706.08840,Usa buffer de ejemplos previos para proyectar gradientes y evitar interferencia.,"Accuracy, BWT, FWT","MNIST, CIFAR-10, SVHN",
Chaudhry,2018,Average GEM,A-GEM,Replay,https://arxiv.org/abs/1812.00420,"Versión eficiente de GEM con proyección promedio de gradientes.,Evalúa límites de rendimiento con buffers extremadamente pequeños.","Accuracy, BWT, FWT, Forgetting","Split MNIST, Permuted MNIST, Tiny ImageNet",
Shin,2017,Deep Generative Replay,DGR,Replay,https://arxiv.org/abs/1705.08690,Genera ejemplos sintéticos para evitar olvidar datos previos.,Accuracy,"MNIST, CIFAR-10",
Hou,2019,Learning a Unified Classifier Incrementally via Rebalancing,LUCIR,Replay,https://sci-hub.se/https://ieeexplore.ieee.org/document/8953661,"Top-,Usa distillation para preservar salidas antiguas mientras entrena nuevas tareas. Regularización ligera para tareas sucesivas sin almacenar ejemplos., Top-,Rebalancea representaciones antiguas y nuevas con distillation adaptativa.,,Maximiza transferencia positiva mediante meta-optimización secuencial.,Evalúa CL entre dominios textuales distintos.","Average Accuracy, Forgetting",CIFAR-100,
Buzzega,2020,Dark Experience Replay,DER,Replay,https://arxiv.org/pdf/2004.07211,Replay sin memoria explícita; usa batch normalization histórico.,"Accuracy, BWT, FWT","Split CIFAR-10, Split CIFAR-100",
Chaudhry,2019,Continual Learning with Tiny Episodic Memories,TinyReplay,Replay,https://arxiv.org/pdf/1902.10486,"Versión eficiente de GEM con proyección promedio de gradientes.,Evalúa límites de rendimiento con buffers extremadamente pequeños.","Accuracy, BWT, FWT","Split CIFAR-10, Split CIFAR-100",
Bang,2021,Rainbow Memory: Efficient Replay for CL,Rainbow,Replay,https://openaccess.thecvf.com/content/CVPR2021/papers/Bang_Rainbow_Memory_Continual_Learning_With_a_Memory_of_Diverse_Samples_CVPR_2021_paper.pdf,Combina priorización y reducción de redundancia en el buffer.,"Accuracy, BWT",CIFAR-100,
Iscen,2020,Memory Efficient Replay using k-Means Prototypes,MER,Replay,https://arxiv.org/abs/2004.00713,Usa prototipos generados por clustering para reducir memoria.,"mAP, Accuracy","iCIFAR-100, ImageNet-Subset",
Wu,2018,Memory Replay GANs,MR-GAN,Replay,https://arxiv.org/pdf/1809.02058,Genera ejemplos con GANs para replay sin almacenamiento real.,"Accuracy, Class Balanced Accuracy","CIFAR-100, ImageNet-Subset",
Kim et al.,2021,Continual Learning on Noisy Data Streams via Self-Purified Replay,Prototype Replay,Replay,https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Continual_Learning_on_Noisy_Data_Streams_via_Self-Purified_Replay_ICCV_2021_paper.pdf,Rehearsal con prototipos suavizados; mejora consistencia inter-tareas.,ACC; FWT,CIFAR-100,
Prabhu et al.,2020,GDumb: A Simple Approach That Works Surprisingly Well,Random Replay Baseline,Replay,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470511.pdf,Reentrena desde cero con buffer aleatorio; fuerte baseline.,ACC; BWT,CIFAR-100,
Tao et al.,2020,Few-Shot Class-Incremental Learning,Replay + Distillation,Replay,https://openaccess.thecvf.com/content_CVPR_2020/papers/Tao_Few-Shot_Class-Incremental_Learning_CVPR_2020_paper.pdf,Distillation multi-clase con embeddings discriminativos; reduce forgetting.,ACC; BWT,CIFAR-100; ImageNet,
Rostami et al.,2023,Robust internal representations for domain generalization,Generative Replay + Contrastive Learning,Replay,https://onlinelibrary.wiley.com/doi/10.1002/aaai.12137,Combina distillation contrastiva y replay generativo; 70–90 % retención.,ACC; Forgetting,MNIST; SVHN,
Fini et al.,2023,Self-Supervised Models are Continual Learners,Self-Supervised Contrastive Replay,Replay,https://arxiv.org/pdf/2112.04215,Integra MoCo v2 + replay; mejora +7 % ACC frente a baselines.,"ACC, Forgetting",CIFAR-100,
Cignoni et al.,2025,CLA: LATENT ALIGNMENT FOR ONLINE CONTINUAL,Latent Replay + SSL,Replay,https://arxiv.org/pdf/2507.10434,Replay de representaciones en lugar de datos; eficiente.,"ACC, BWT",CIFAR-100,
Hickok,2025,Scalable Strategies for Continual Learning with Replay,Mixed-task Replay,Replay,https://arxiv.org/abs/2505.12512,Mezcla tareas pasadas y nuevas para mejorar generalización; +6 % ACC.,"ACC, Forgetting",CIFAR-100,
Rajasegaran et al.,2021,ITL: Incremental Task Learning via Meta-Learning,Meta-Learning Continual,Optimization,https://arxiv.org/abs/2003.11652,Integra MAML con CL; +6 % vs GEM/A-GEM.,"ACC, BWT","CIFAR-100, MiniImageNet",
Srinivas et al.,2020,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,Self-Supervised + Contrastive,Optimization,https://arxiv.org/abs/2004.04136,CL sin etiquetas; mejora robustez y generalización.,"ACC, FWT","STL-10, CIFAR-100",
Ahn et al.,2022,SS-IL++: Improved Self-Supervised Incremental Learning,Self-Supervised Incremental Learning,Optimization,https://openaccess.thecvf.com/content/ICCV2021/papers/Ahn_SS-IL_Separated_Softmax_for_Incremental_Learning_ICCV_2021_paper.pdf,Pseudoetiquetas contrastivas y augmentations; sin etiquetas.,"ACC, Forgetting","CIFAR-100, STL-10",
Pham et al,2021,"DualNet: Continual Learning, Fast and Slow",Dual-branch network (plastic/stable),Optimization,https://arxiv.org/abs/2110.00175,Combina aprendizajes rápidos/lentos para equilibrio estabilidad–plasticidad.,"ACC, Forgetting",Split CIFAR-100,
Saha et al.,2021,Gradient Projection Memory (GPM),Proyección de gradientes en subespacios SVD,Optimization-Based,https://arxiv.org/abs/2103.09762,Restringe actualización a subespacio relevante; reduce olvido ≈ 50 %.,"ACC, BWT",Split CIFAR-100,
_apacz et al.,2021,Exploring the Stability Gap in Continual Learning,Prototype-based + Distillation,Optimization,https://openaccess.thecvf.com/content/WACV2025/papers/Lapacz_Exploring_the_Stability_Gap_in_Continual_Learning_The_Role_of_WACV_2025_paper.pdf,Clasifica por media incremental; robusto en ViT y CNN.,ACC; Forgetting,CIFAR-100,
Madaan et al.,2022,Representational Continuity for Continual Learning,Contrastive Representation Regularization,Optimization,https://arxiv.org/abs/2110.06976,Mantiene coherencia de representaciones entre tareas.,"ACC, Cosine Sim",CIFAR-100,
Budden,2020,Orthogonal Gradient Descent for Continual Learning,OGD,Optimización,https://arxiv.org/abs/2006.05964,"Presenta las G-GLN, redes sin backpropagation que aprenden localmente y son robustas al olvido catastrófico.","RMSE, MSE, Regression Error","UCI Regression Benchmarks, SARCOS, MNIST, Swiss Roll",
Kolouri,2021,Gradient Projection Memory for Continual Learning,GPM,Optimización,https://openreview.net/forum?id=BJge3TNKwH,,"Accuracy, Reconstruction Error","MNIST, Omniglot",
Riemer,2019,Learning to Learn without Forgetting by Maximizing Transfer,Meta-CL,Optimización,https://arxiv.org/pdf/1810.11910,Maximiza transferencia positiva mediante meta-optimización secuencial.,"Accuracy, Forgetting","Permuted MNIST, Split CIFAR-10",
He,2021,Overcoming Catastrophic Interference by Conceptors,CAL,Optimización,https://arxiv.org/pdf/1707.04853,"Penaliza cambios en pesos importantes usando Fisher Information. Propone cálculo mejorado del Fisher para estabilidad inter-tareas.,Controla interferencia con proyecciones conceptuales sobre pesos.,Analiza efecto del learning rate en estabilidad-plasticidad.,Modula gradientes por tarea para reducir interferencia local.,,Poda iterativa y reasignación de pesos para reutilizar capacidad.,,Aísla parámetros relevantes por tarea usando atención binaria.",Accuracy,CIFAR-10,
Hall,2025,Optimization of path-integral tensor-multiplication schemes in open quantum systems,PI-Opt,Optimización,https://arxiv.org/html/2502.15136v1,"--,Estudia desafíos de CL online y propone métricas estandarizadas.",Accuracy,MNIST,
Mirzadeh,2020,Understanding the Role of Learning Rate in CL,LR-Study,Optimización,https://arxiv.org/pdf/2006.06958,Analiza efecto del learning rate en estabilidad-plasticidad.,"Accuracy, Average Accuracy",Split CIFAR-100,
Ebrahimi,2020,Adversarial Continual Learning,UCL,Optimización,https://arxiv.org/pdf/2003.09553,Ajusta gradientes según incertidumbre estimada por Bayesian dropout.,Accuracy,"CIFAR-10, CIFAR-100",
Son,2020,When Meta-Learning Meets Online and Continual Learning: A Survey,MetaOpt,Optimización,https://arxiv.org/html/2311.05241v3,Evalúa mecanismos de memoria episódica en modelos secuenciales.,Accuracy,Split MNIST,
Pan,2020,Personalized Federated Learning via Gradient Modulation for Heterogeneous Text Summarization,TGM,Optimización,https://arxiv.org/abs/2304.11524,"Adapta optimizador global mediante aprendizaje meta-recurrente.,,Expande la red dinámicamente al detectar saturación de capacidad.,,Usa sparsity dinámica para expandir capacidad sin interferencia.",Accuracy,CIFAR-10,
Javed,2019,Meta-Learning Representations for Continual Learning,MetaRep,Optimización,https://arxiv.org/abs/1905.12588,Aprende representaciones transferibles entre tareas sucesivas.,"Accuracy, BWT, FWT",Split CIFAR-100,
Rusu,2016,Progressive Neural Networks,PNN,Arquitecturas,https://arxiv.org/abs/1606.04671,"Transfer,Añade columnas de red por tarea, congelando pesos anteriores.",Accuracy,"iCIFAR-100, iImageNet",
Mallya & Lazebnik,2018,PackNet,PackNet,Arquitecturas,https://openaccess.thecvf.com/content_cvpr_2018/html/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.html,Poda iterativa y reasignación de pesos para reutilizar capacidad.,"Accuracy, Task Incremental Accuracy","Split MNIST, Split CIFAR-100",
Mallya,2018,Piggyback,Piggyback,Arquitecturas,https://arxiv.org/abs/1801.06519,"Poda iterativa y reasignación de pesos para reutilizar capacidad. Top-,Máscaras binarias sobre red base preentrenada para nuevas tareas.",Accuracy,Split MNIST,
Serr√†,2018,Hard Attention to the Task (HAT),HAT,Arquitecturas,https://arxiv.org/abs/1801.01423,,Accuracy,CIFAR-100,
Yoon,2018,Dynamic Expansion Networks,DEN,Arquitecturas,https://arxiv.org/abs/1708.01547,Expande la red dinámicamente al detectar saturación de capacidad.,"Accuracy, BWT, FWT","Split MNIST, Split CIFAR-100",
Fernando,2017,PathNet,PathNet,Arquitecturas,https://arxiv.org/abs/1701.08734,"Transfer,Evoluciona rutas neuronales fijas para cada tarea.",Accuracy,"MNIST, CIFAR-10",
WU,2025,"Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution",MaskRoute,Arquitecturas,https://arxiv.org/html/2510.01997v2,Genera ejemplos con GANs para replay sin almacenamiento real.,"Accuracy, Forgetting",CIFAR-100,
Sokar,2021,Avoiding Forgetting and Allowing Forward Transfer in Continual Learning via Sparse Networks,SparseNet,Arquitecturas,https://arxiv.org/abs/2110.05329,,Accuracy,Split MNIST,
Abati,2020,Conditional Computation in CL,CondComp,Arquitecturas,https://arxiv.org/pdf/2004.00070,Activa dinámicamente módulos según tarea actual.,"Accuracy, FWT, BWT","CIFAR-10, CIFAR-100",
Yang,2022,Dynamic Sparsity Networks for Incremental Tasks,DSN,Arquitecturas,https://arxiv.org/pdf/1909.06964,,Accuracy,Split MNIST,
Zhang et al.,2024,IMPROVING DESIGN OF INPUT CONDITION INVARIANT SPEECH ENHANCEMENT,Prompted MIM Pretraining,Arquitecturas,arXiv 2024,Combina MIM y prompts en ViTs incrementales; mejora +7 % retención.,"ACC, BWT",ImageNet-100,
Liu et al.,2021,AANets: Adaptive Aggregation Networks for Continual Learning,Arquitecturas (Attention Aggregation),Arquitecturas,https://arxiv.org/abs/2010.05063,Agrega bloques estables/plásticos de forma dinámica con atención.,ACC; _Old/New,CIFAR-100,
Zhao et al.,2023,PromptFusion: Cross-Modal Continual Learning with Fusion Prompts,Vision-Language Prompt Tuning,Arquitecturas,https://aclanthology.org/2023.repl4nlp-1.10/,Fusiona prompts multimodales texto-imagen; mejora +5 % ACC.,"ACC, BWT","COCO, VG, ImageNet",
Lomonaco et al.,2022,Avalanche: An End-to-End Library for Continual Learning,Framework modular,Arquitecturas,https://arxiv.org/abs/2104.00405,Establece protocolos y métricas estandarizadas de CL.,"ACC, FWT, BWT","Split MNIST, CORe50",
Sung et al.,2022,VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks,PEFT + Cross-modal tuning,Arquitecturas,https://arxiv.org/abs/2112.06825,Adapta CLIP modularmente; 92 % retención.,"ACC, Params Ratio","COCO, VG",
Huai et al.,2025,CL-MoE: Enhancing Multimodal Large Language Model,Modular Routing Networks,Arquitecturas,https://arxiv.org/html/2503.00413v1,Asigna expertos por tarea; sin interferencia entre dominios.,"ACC, Params Count","CIFAR-100, ImageNet",
Wang et al.,2024,Multi-modal Learning with Missing Modality via Shared-Specific Feature Modelling,Cross-modal Adaptation,Arquitecturas,https://arxiv.org/abs/2307.14126,CL texto-imagen con replay compartido.,"ACC, F1","COCO, Flickr30K",
Lou et al.,2023,LADA: Scalable Label-Specific CLIP Adapter for Continual Learning,Adapter tuning for CLIP,Arquitecturas,https://arxiv.org/html/2505.23271v1,Incorpora adaptadores ligeros en CLIP; 95 % retención con 10 % de parámetros.,"ACC, Params Ratio","COCO, ImageNet",
Parisi,2019,Continual Lifelong Learning with Neural Networks: A Review,Survey,Benchmarks/Surveys,https://arxiv.org/abs/1802.07569,Revisión general de metodologías y taxonomía del CL.,"Accuracy, FWT, BWT","CORe50, iCubWorld28",
De Lange,2021,A Continual Learning Survey: Defying Forgetting,Survey,Benchmarks/Surveys,https://arxiv.org/abs/1909.08383,Analiza y categoriza más de papers sobre CL experimental.,"Accuracy, Forgetting, BWT","Split MNIST, CORe50, CIFAR-100",
Caccia,2023,Online Continual Learning: Challenges, Evaluation and Solutions,Survey,Benchmarks/Surveys,"--,Estudia desafíos de CL online y propone métricas estandarizadas.","Accuracy, BWT","Split MNIST, Split CIFAR-100",
Hadsell,2020,Embracing Change: Continual Learning in Deep Neural Networks,Review,Benchmarks/Surveys,https://www.sciencedirect.com/science/article/pii/S1364661320302199,Revisión conceptual sobre plasticidad y estabilidad en CL.,Accuracy,"MNIST, Omniglot",
Zhou,2021,Class-Incremental Learning: A Survey,Survey,Benchmarks/Surveys,https://arxiv.org/html/2302.03648v2,Evalúa CL entre dominios textuales distintos.,Accuracy,CIFAR-10,
Fields,2023,Vision Language Transformers: A Survey,Survey,Benchmarks/Surveys,https://arxiv.org/abs/2307.03254,,Accuracy,CIFAR-10,
Maltoni,2019,CORe50: Benchmark for Continuous Object Recognition,Benchmark,Benchmarks/Surveys,https://arxiv.org/pdf/1705.03550,Presenta benchmark CORe50 para reconocimiento continuo.,Accuracy,CORe50,
De Lange,2021,Continual evaluation for lifelong learning: Identifying the stability gap,Survey,Benchmarks/Surveys,https://arxiv.org/abs/2205.13452,Analiza y categoriza más de papers sobre CL experimental.,"Accuracy, Forgetting","Split MNIST, CIFAR-100",
Bell,2025,The Future of continual learning,Survey,Benchmarks/Surveys,https://arxiv.org/pdf/2506.03320,,Accuracy,MNIST,
DeMasson,2019,Episodic Memory in LSTM Language Models,Survey,Benchmarks/Surveys,https://arxiv.org/abs/1906.01076,Evalúa mecanismos de memoria episódica en modelos secuenciales.,Accuracy,Split MNIST,
Ke,2021,Continual Learning for Natural Language Processing: A Survey,Survey,NLP,https://arxiv.org/pdf/2211.12701,"Asigna importancia dinámica a pesos según su contribución al desempeño previo.,,Modela representaciones latentes bayesianas para estabilidad y plasticidad.","Accuracy, Perplexity","GLUE, XSum, CoNLL-2003",
Hu,2022,LoRA: Low-Rank Adaptation of Large Language Models,LoRA,NLP,https://arxiv.org/pdf/2106.09685,"Introduce capas de bajo rango para ajustar grandes LLM sin olvidar.,,Replay aleatorio como baseline fuerte para CL.",Accuracy,CIFAR-10,
Wu,2024,Continual Learning for Large Language Models,CL-LLM,NLP,https://arxiv.org/pdf/2402.01364v1,Genera ejemplos con GANs para replay sin almacenamiento real.,Accuracy,CIFAR-100,
Sun,2020,LAMOL: Language Modeling for Lifelong Learning,LAMOL,NLP,https://arxiv.org/abs/1909.03329,Reformula tareas como modelado de lenguaje y usa replay textual.,Accuracy,CIFAR-10,
Ke,2023,CONTINUAL PRE-TRAINING OF LANGUAGE MODELS,CL-BERT,NLP,https://arxiv.org/pdf/2302.03241,"Asigna importancia dinámica a pesos según su contribución al desempeño previo.,,Modela representaciones latentes bayesianas para estabilidad y plasticidad.","Accuracy, Perplexity","GLUE, XSum, CoNLL-2003",
Pink,2025,Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents,Episodic,NLP,https://arxiv.org/pdf/2502.06975,,Accuracy,MNIST,
Wang,2021,Few-shot Continual Learning: a Brain-inspired Approach,MetaCL,NLP,https://arxiv.org/abs/2104.09034,Combina meta-learning y CL para generalización rápida. Resume líneas recientes de investigación experimental en CL.,Accuracy,CIFAR-10,
Lu,2024,Visual Prompt Tuning in Null Space for Continual Learning,PromptCL,NLP,https://arxiv.org/pdf/2406.05658,"Top-,Rebalancea representaciones antiguas y nuevas con distillation adaptativa.,Usa prototipos generados por clustering para reducir memoria., Transfer,Añade columnas de red por tarea, congelando pesos anteriores., Transfer,Evoluciona rutas neuronales fijas para cada tarea.,,Introduce capas de bajo rango para ajustar grandes LLM sin olvidar.,,Usa prompt tuning incremental para adaptar LMs a nuevas tareas. --,Estudia desafíos de CL online y propone métricas estandarizadas.,,Analiza prácticas de evaluación en CL aplicado.",Accuracy,CIFAR-10,
Li,2025,Dynamic Integration of Task-Specific Adapters for Class Incremental Learning,DynAdapt,NLP,https://arxiv.org/abs/2409.14983,"Penaliza cambios en pesos importantes usando Fisher Information.,,Asigna importancia dinámica a pesos según su contribución al desempeño previo.,,Evalúa sensibilidad de salida para regularizar parámetros relevantes., Top-,Usa distillation para preservar salidas antiguas mientras entrena nuevas tareas. Propone cálculo mejorado del Fisher para estabilidad inter-tareas.,Mide importancia de pesos con caminata aleatoria entre tareas. Regularización ligera para tareas sucesivas sin almacenar ejemplos.,,Modela representaciones latentes bayesianas para estabilidad y plasticidad.,,Replay sin memoria explícita; usa batch normalization histórico.,Evalúa límites de rendimiento con buffers extremadamente pequeños.,,Fuerza ortogonalidad entre gradientes de tareas para evitar interferencia.,Optimiza trayectorias de parámetros con regularización continua.,Analiza efecto del learning rate en estabilidad-plasticidad.,,Adapta optimizador global mediante aprendizaje meta-recurrente.,,Poda iterativa y reasignación de pesos para reutilizar capacidad.,,Aísla parámetros relevantes por tarea usando atención binaria. Aplica enrutamiento dinámico con máscaras entre módulos.,Selecciona subconjuntos de pesos escasos para cada tarea.,,Reformula tareas como modelado de lenguaje y usa replay textual.,,Preentrena BERT secuencialmente con distillation intertareas.,,Combina meta-learning y CL para generalización rápida.,,Usa prompt tuning incremental para adaptar LMs a nuevas tareas. Revisión general de metodologías y taxonomía del CL. Analiza y categoriza más de papers sobre CL experimental. --,Estudia desafíos de CL online y propone métricas estandarizadas. Revisión conceptual sobre plasticidad y estabilidad en CL.,,Analiza prácticas de evaluación en CL aplicado. Resume líneas recientes de investigación experimental en CL.,,Replay aleatorio como baseline fuerte para CL.,,Aprendizaje incremental sin etiquetas de tarea.",Accuracy,"CIFAR-10, CIFAR-100",
Zhou,2023,Class-Incremental Learning: A Survey,Domain-IL,NLP,https://arxiv.org/pdf/2302.03648,Evalúa CL entre dominios textuales distintos.,Accuracy,CIFAR-10,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,